"""
æµå¼ä»»åŠ¡å¤„ç†å™¨
===============

å®Œå…¨æµå¼æ¶æ„ - æ‰€æœ‰æ­¥éª¤çš„è¾“å‡ºéƒ½é€šè¿‡ SSE å®æ—¶è¿”å›
æ”¯æŒæ–­ç‚¹ç»­ä¼ ï¼šæµå¼æ•°æ®åŒæ—¶å­˜å…¥ Redis
"""

import asyncio
import os  # ç”¨äºè¯»å–ç¯å¢ƒå˜é‡
import json
import traceback
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
from typing import Optional, List, Dict, Any, Callable, Awaitable
import pandas as pd
import numpy as np

from app.core.session import Session, Message
from app.core.redis_client import get_redis
from app.schemas.session_schema import (
    TimeSeriesPoint,
    UnifiedIntent,
    ResolvedKeywords,
    StockMatchResult,  # ä¿ç•™ä»¥å…¼å®¹
    RegionMatchResult,
    SummarizedNewsItem,
)

# Services
from app.services.stock_matcher import get_stock_matcher  # ä¿ç•™ä»¥å…¼å®¹
from app.services.region_matcher import get_region_matcher
from app.services.rag_client import check_rag_availability
from app.services.influence_analyzer import InfluenceAnalyzer
from app.data.industry_structure_client import get_industry_structure_client
from app.services.stock_signal_service import StockSignalService

# Agents
from app.agents import (
    IntentAgent,
    ReportAgent,
    ErrorExplainerAgent,
    SentimentAgent,
    NewsSummaryAgent,
    PredictionAnalysisAgent,
)
from app.services.trend_service import TrendService

# Data clients
from app.data.rag_searcher import RAGSearcher

# Data & Models
from app.data import extract_domain
from app.data.fetcher import DataFetchError
from app.models import TimeSeriesAnalyzer

# Workflows
from app.core.workflows import (
    fetch_power_data,
    fetch_news_all,
    fetch_rag_reports,
    search_web,
    search_news_around_date,
    fetch_domain_news,
    run_forecast,
    df_to_points,
    recommend_forecast_params,
    select_best_model,
)


class StreamingTaskProcessor:
    """
    æµå¼ä»»åŠ¡å¤„ç†å™¨

    æ ¸å¿ƒæµç¨‹ï¼ˆå…¨ç¨‹æµå¼ï¼‰:
    1. æ„å›¾è¯†åˆ« - æµå¼è¿”å›æ€è€ƒè¿‡ç¨‹
    2. è‚¡ç¥¨éªŒè¯ - è¿”å›åŒ¹é…ç»“æœ
    3. æ•°æ®è·å– - è¿”å›å†å²æ•°æ®å’Œæ–°é—»
    4. åˆ†æå¤„ç† - è¿”å›ç‰¹å¾å’Œæƒ…ç»ª
    5. æ¨¡å‹é¢„æµ‹ - è¿”å›é¢„æµ‹ç»“æœ
    6. æŠ¥å‘Šç”Ÿæˆ - æµå¼è¿”å›æŠ¥å‘Šå†…å®¹
    """

    # Baseline æƒ©ç½šæœºåˆ¶å¼€å…³
    # True: å¯ç”¨æƒ©ç½šæœºåˆ¶ï¼Œç”¨æˆ·æŒ‡å®šæ¨¡å‹ä¸å¦‚ baseline æ—¶é™çº§ä¸º baseline
    # False: ç¦ç”¨æƒ©ç½šæœºåˆ¶ï¼Œå³ä½¿æœ€ä½³æ¨¡å‹ä¸å¦‚ baseline ä¹Ÿä½¿ç”¨æœ€ä½³æ¨¡å‹
    ENABLE_BASELINE_PENALTY = False

    def __init__(self):
        self.intent_agent = IntentAgent()
        self.rag_searcher = RAGSearcher()
        self.report_agent = ReportAgent()
        self.error_explainer = ErrorExplainerAgent()
        self.sentiment_agent = SentimentAgent()
        self.news_summary_agent = NewsSummaryAgent()
        self.stock_matcher = get_stock_matcher()  # ä¿ç•™ä»¥å…¼å®¹
        self.region_matcher = get_region_matcher()
        self.prediction_analysis_agent = PredictionAnalysisAgent()
        self.redis = get_redis()

    async def execute_streaming(
        self,
        session_id: str,
        message_id: str,
        user_input: str,
        event_queue: asyncio.Queue | None,
        model_name: Optional[str] = None,
    ):
        """
        æ‰§è¡Œå®Œå…¨æµå¼ä»»åŠ¡

        Args:
            session_id: ä¼šè¯ ID
            message_id: æ¶ˆæ¯ ID
            user_input: ç”¨æˆ·è¾“å…¥
            event_queue: äº‹ä»¶é˜Ÿåˆ—ï¼ˆå‘é€åˆ° SSEï¼Œåå°ä»»åŠ¡æ—¶ä¸º Noneï¼‰
            model_name: é¢„æµ‹æ¨¡å‹åç§°
        """
        session = Session(session_id)
        message = Message(message_id, session_id)

        # è®¾ç½®æµå¼çŠ¶æ€
        self._update_stream_status(message, "streaming")

        try:
            conversation_history = session.get_conversation_history()

            # === Step 1: æ„å›¾è¯†åˆ«ï¼ˆæµå¼ï¼‰ ===
            await self._emit_event(
                event_queue,
                message,
                {"type": "step_start", "step": 1, "step_name": "æ„å›¾è¯†åˆ«"},
            )

            message.update_step_detail(1, "running", "åˆ†æç”¨æˆ·æ„å›¾...")

            intent, thinking_content = await self._step_intent_streaming(
                user_input, conversation_history, event_queue, message
            )

            if not intent:
                await self._emit_error(event_queue, message, "æ„å›¾è¯†åˆ«å¤±è´¥")
                return

            # å¦‚æœç”¨æˆ·é€šè¿‡ API æŒ‡å®šäº†æ¨¡å‹ï¼Œè¦†ç›–æ„å›¾è¯†åˆ«çš„ç»“æœ
            # print(f"[ModelSelection] API ä¼ å…¥çš„ model_name: {model_name}")
            # print(f"[ModelSelection] æ„å›¾è¯†åˆ«è¿”å›çš„ forecast_model: {intent.forecast_model}")
            if model_name is not None:
                intent.forecast_model = model_name
                # print(f"[ModelSelection] ä½¿ç”¨ API æŒ‡å®šçš„æ¨¡å‹: {model_name}")
            else:
                # å¦‚æœç”¨æˆ·æ²¡æœ‰é€šè¿‡ API æŒ‡å®šæ¨¡å‹ï¼Œä¸” LLM è¿”å›çš„æ˜¯ "prophet"ï¼ˆå¯èƒ½æ˜¯é»˜è®¤å€¼ï¼‰ï¼Œ
                # åˆ™å°†å…¶è®¾ä¸º Noneï¼Œè§¦å‘è‡ªåŠ¨æ¨¡å‹é€‰æ‹©
                intent.forecast_model = None

            # ä¿å­˜æ„å›¾
            message.save_unified_intent(intent)
            message.append_thinking_log("intent", "æ„å›¾è¯†åˆ«", thinking_content)

            # å‘é€æ„å›¾ç»“æœ
            await self._emit_event(
                event_queue,
                message,
                {
                    "type": "intent",
                    "intent": "forecast" if intent.is_forecast else "chat",
                    "is_forecast": intent.is_forecast,
                    "reason": intent.reason,
                },
            )

            # å¤„ç†è¶…å‡ºèŒƒå›´
            if not intent.is_in_scope:
                reply = (
                    intent.out_of_scope_reply
                    or "æŠ±æ­‰ï¼Œæˆ‘æ˜¯é‡‘èæ—¶åºåˆ†æåŠ©æ‰‹ï¼Œæš‚ä¸æ”¯æŒæ­¤ç±»é—®é¢˜ã€‚"
                )
                message.save_conclusion(reply)
                message.update_step_detail(1, "completed", "è¶…å‡ºæœåŠ¡èŒƒå›´")
                message.mark_completed()
                self._update_stream_status(message, "completed")
                await self._emit_event(
                    event_queue,
                    message,
                    {"type": "chat_chunk", "content": reply, "is_complete": True},
                )
                await self._emit_done(event_queue, message)
                return

            await self._emit_event(
                event_queue,
                message,
                {
                    "type": "step_complete",
                    "step": 1,
                    "data": {"intent": "forecast" if intent.is_forecast else "chat"},
                },
            )
            message.update_step_detail(
                1, "completed", f"æ„å›¾: {'é¢„æµ‹' if intent.is_forecast else 'å¯¹è¯'}"
            )

            # === Step 2: åŒºåŸŸéªŒè¯ ===
            region_match_result = None
            stock_match_result = None  # ä¿ç•™ä»¥å…¼å®¹
            resolved_keywords = None

            # ä¼˜å…ˆä½¿ç”¨region_mentionï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨stock_mentionï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
            region_mention = intent.region_mention or intent.stock_mention

            if region_mention:
                await self._emit_event(
                    event_queue,
                    message,
                    {"type": "step_start", "step": 2, "step_name": "åŒºåŸŸéªŒè¯"},
                )

                query_name = (
                    intent.region_name or intent.stock_full_name or region_mention
                )
                message.update_step_detail(2, "running", f"éªŒè¯åŒºåŸŸ: {query_name}")

                region_match_result = await asyncio.to_thread(
                    self.region_matcher.match, query_name
                )

                if not region_match_result or not region_match_result.matched:
                    error_msg = f"æœªæ‰¾åˆ°åŒºåŸŸã€Œ{query_name}ã€ï¼Œè¯·æ£€æŸ¥åŒºåŸŸåç§°æ˜¯å¦æ­£ç¡®ã€‚æ”¯æŒçš„åŒºåŸŸ: åŒ—äº¬ã€ä¸Šæµ·ã€å¹¿å·ã€æ·±åœ³ã€æ­å·ã€æˆéƒ½ã€æ­¦æ±‰ã€è¥¿å®‰ã€å—äº¬ã€å¤©æ´¥"
                    message.save_conclusion(error_msg)
                    message.update_step_detail(2, "error", error_msg)
                    message.mark_completed()
                    self._update_stream_status(message, "error")
                    await self._emit_error(event_queue, message, error_msg)
                    return

                region_info = region_match_result.region_info
                resolved_keywords = self.intent_agent.resolve_keywords(
                    intent,
                    region_name=region_info.region_name if region_info else None,
                    region_code=region_info.region_code if region_info else None,
                )
                message.save_resolved_keywords(resolved_keywords)

                await self._emit_event(
                    event_queue,
                    message,
                    {
                        "type": "step_complete",
                        "step": 2,
                        "data": {
                            "region_code": region_info.region_code
                            if region_info
                            else "",
                            "region_name": region_info.region_name
                            if region_info
                            else "",
                        },
                    },
                )
                message.update_step_detail(
                    2,
                    "completed",
                    f"åŒ¹é…: {region_info.region_name}({region_info.region_code})"
                    if region_info
                    else "æ— åŒ¹é…",
                )
            else:
                resolved_keywords = ResolvedKeywords(
                    search_keywords=intent.raw_search_keywords,
                    rag_keywords=intent.raw_rag_keywords,
                    domain_keywords=intent.raw_domain_keywords,
                )

            # === æ ¹æ®æ„å›¾æ‰§è¡Œä¸åŒæµç¨‹ ===
            if intent.is_forecast:
                await self._execute_forecast_streaming(
                    message,
                    session,
                    user_input,
                    intent,
                    region_match_result,  # ä½¿ç”¨region_match_result
                    resolved_keywords,
                    conversation_history,
                    event_queue,
                )
            else:
                await self._execute_chat_streaming(
                    message,
                    session,
                    user_input,
                    intent,
                    region_match_result,  # ä½¿ç”¨region_match_result
                    resolved_keywords,
                    conversation_history,
                    event_queue,
                )

            # æ ‡è®°å®Œæˆ
            message.mark_completed()
            self._update_stream_status(message, "completed")

            # æ·»åŠ åŠ©æ‰‹å›å¤åˆ°å¯¹è¯å†å²
            data = message.get()
            if data and data.conclusion:
                session.add_conversation_message("assistant", data.conclusion)

            await self._emit_done(event_queue, message)

        except Exception as e:
            print(f"âŒ Streaming task error: {traceback.format_exc()}")
            message.mark_error(str(e))
            self._update_stream_status(message, "error")
            await self._emit_error(event_queue, message, str(e))

    # ========== æµå¼æ„å›¾è¯†åˆ« ==========

    async def _step_intent_streaming(
        self,
        user_input: str,
        conversation_history: List[dict],
        event_queue: asyncio.Queue | None,
        message: Message,
    ) -> tuple:
        """æµå¼æ„å›¾è¯†åˆ«"""
        import queue as thread_queue

        chunk_queue: thread_queue.Queue = thread_queue.Queue()

        def on_chunk(chunk: str):
            """åŒæ­¥å›è°ƒ - æ”¾å…¥çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—"""
            chunk_queue.put(chunk)

        def run_intent():
            """åœ¨çº¿ç¨‹ä¸­è¿è¡Œæ„å›¾è¯†åˆ«"""
            result = self.intent_agent.recognize_intent_streaming(
                user_input, conversation_history, on_chunk
            )
            chunk_queue.put(None)  # ç»“æŸæ ‡è®°
            return result

        # å¯åŠ¨çº¿ç¨‹ä»»åŠ¡
        loop = asyncio.get_running_loop()
        future = loop.run_in_executor(None, run_intent)

        # è½®è¯¢é˜Ÿåˆ—ï¼Œé€šè¿‡ _emit_event å‘é€äº‹ä»¶
        thinking_content = ""
        while True:
            try:
                chunk = chunk_queue.get_nowait()
                if chunk is None:
                    break
                thinking_content += chunk
                await self._emit_event(
                    event_queue,
                    message,
                    {"type": "thinking", "content": thinking_content},
                )
            except thread_queue.Empty:
                if future.done():
                    # å¤„ç†å‰©ä½™çš„ chunks
                    while not chunk_queue.empty():
                        chunk = chunk_queue.get_nowait()
                        if chunk is not None:
                            thinking_content += chunk
                            await self._emit_event(
                                event_queue,
                                message,
                                {"type": "thinking", "content": thinking_content},
                            )
                    break
                await asyncio.sleep(0.01)

        intent, final_thinking = await future
        return intent, final_thinking or thinking_content

    # ========== é¢„æµ‹æµç¨‹ï¼ˆæµå¼ï¼‰ ==========

    async def _execute_forecast_streaming(
        self,
        message: Message,
        session: Session,
        user_input: str,
        intent: UnifiedIntent,
        region_match: Optional[RegionMatchResult],
        keywords: ResolvedKeywords,
        conversation_history: List[dict],
        event_queue: asyncio.Queue | None,
    ):
        """æµå¼é¢„æµ‹æµç¨‹"""
        region_info = region_match.region_info if region_match else None
        region_name = region_info.region_name if region_info else user_input
        region_code = region_info.region_code if region_info else ""

        # === Step 3: æ•°æ®è·å– ===
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_start", "step": 3, "step_name": "æ•°æ®è·å–"},
        )
        message.update_step_detail(3, "running", "è·å–å†å²æ•°æ®å’Œæ–°é—»...")

        # é™åˆ¶å†å²å¤©æ•°ï¼Œç¡®ä¿ä¸è¶…è¿‡Open-Meteo APIé™åˆ¶ï¼ˆ92å¤©ï¼‰
        # åŒæ—¶ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ•°æ®ç”¨äºæ¨¡å‹è®­ç»ƒ
        effective_history_days = min(intent.history_days, 92)
        effective_history_days = max(effective_history_days, 30)  # è‡³å°‘30å¤©ç”¨äºè®­ç»ƒ

        # ä½¿ç”¨åŒ—äº¬æ—¶åŒºç¡®ä¿ä¸€è‡´æ€§
        BEIJING_TZ = ZoneInfo("Asia/Shanghai")
        now = datetime.now(BEIJING_TZ)
        end_date = now.strftime("%Y%m%d")
        start_date = (now - timedelta(days=effective_history_days)).strftime("%Y%m%d")

        # å¹¶è¡Œè·å–æ•°æ®
        power_data_task = asyncio.create_task(
            fetch_power_data(region_name, start_date, end_date, effective_history_days)
        )
        news_task = asyncio.create_task(
            fetch_news_all(region_name, intent.history_days)
        )
        rag_available = await check_rag_availability() if intent.enable_rag else False
        rag_task = (
            asyncio.create_task(
                fetch_rag_reports(self.rag_searcher, keywords.rag_keywords)
            )
            if intent.enable_rag and rag_available
            else None
        )

        # ä¼˜å…ˆè·å–ä¾›ç”µéœ€æ±‚æ•°æ®å’Œå¤©æ°”æ•°æ®
        try:
            power_result = await power_data_task
        except Exception as e:
            power_result = e

        # å¤„ç†ä¾›ç”µéœ€æ±‚æ•°æ®
        df = None
        weather_df = None
        if isinstance(power_result, DataFetchError):
            error_explanation = await asyncio.to_thread(
                self.error_explainer.explain_data_fetch_error, power_result, user_input
            )
            message.save_conclusion(error_explanation)
            message.update_step_detail(3, "error", "æ•°æ®è·å–å¤±è´¥")
            news_task.cancel()
            if rag_task:
                rag_task.cancel()
            await self._emit_error(event_queue, message, error_explanation)
            return
        elif isinstance(power_result, Exception):
            error_msg = f"è·å–æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {str(power_result)}"
            message.save_conclusion(error_msg)
            message.update_step_detail(3, "error", "æ•°æ®è·å–å¤±è´¥")
            news_task.cancel()
            if rag_task:
                rag_task.cancel()
            await self._emit_error(event_queue, message, error_msg)
            return
        else:
            # å¤„ç†è¿”å›çš„å…ƒç»„ (ä¾›ç”µæ•°æ®, å¤©æ°”æ•°æ®)
            if isinstance(power_result, tuple):
                df, weather_df = power_result
            else:
                df = power_result
                weather_df = None

        if df is None or df.empty:
            error_msg = (
                f"æ— æ³•è·å– {region_name} çš„å†å²ä¾›ç”µéœ€æ±‚æ•°æ®ï¼Œè¯·æ£€æŸ¥åŒºåŸŸåç§°æ˜¯å¦æ­£ç¡®ã€‚"
            )
            message.save_conclusion(error_msg)
            message.update_step_detail(3, "error", "æ•°æ®è·å–å¤±è´¥")
            news_task.cancel()
            if rag_task:
                rag_task.cancel()
            await self._emit_error(event_queue, message, error_msg)
            return

        # ç«‹å³ä¿å­˜å¹¶å‘é€ä¾›ç”µéœ€æ±‚æ•°æ®
        original_points = df_to_points(df, is_prediction=False)
        message.save_time_series_original(original_points)

        await self._emit_event(
            event_queue,
            message,
            {
                "type": "data",
                "data_type": "time_series_original",
                "data": [p.model_dump() for p in original_points],
            },
        )

        # ç­‰å¾…æ–°é—»å’Œ RAG
        pending_tasks = [news_task]
        if rag_task:
            pending_tasks.append(rag_task)

        other_results = await asyncio.gather(*pending_tasks, return_exceptions=True)

        news_result = (
            other_results[0]
            if not isinstance(other_results[0], Exception)
            else ([], {})
        )
        rag_sources = (
            other_results[1]
            if len(other_results) > 1
            and not isinstance(other_results[1], Exception)
            and intent.enable_rag
            else []
        )

        news_items, sentiment_result = news_result

        # æ€»ç»“æ–°é—» - ç›´æ¥è°ƒç”¨ Agent
        if news_items:
            summarized_news, _ = await asyncio.to_thread(
                self.news_summary_agent.summarize, news_items
            )
        else:
            summarized_news = []

        message.save_news(summarized_news)

        # å‘é€æ–°é—»æ•°æ®
        if summarized_news:
            await self._emit_event(
                event_queue,
                message,
                {
                    "type": "data",
                    "data_type": "news",
                    "data": [n.model_dump() for n in summarized_news],
                },
            )

        if rag_sources:
            message.save_rag_sources(rag_sources)

        # === è®¡ç®—å¼‚å¸¸åŒºåŸŸï¼ˆåœ¨Step 3å®Œæˆå‰ï¼Œç¡®ä¿resumeæ—¶èƒ½è·å–åˆ°ï¼‰===
        print(
            f"[AnomalyZones] Starting dynamic clustering for message {message.message_id}"
        )
        try:
            import pandas as pd
            from app.services.stock_signal_service import StockSignalService
            from app.agents.event_summary_agent import EventSummaryAgent

            # ä» df æå–æ—¥æœŸã€æ”¶ç›˜ä»·ã€æˆäº¤é‡
            sig_df = pd.DataFrame(
                {
                    "date": df["ds"].dt.strftime("%Y-%m-%d"),
                    "close": df["y"],
                    "volume": df.get("volume", [1] * len(df)),
                }
            )

            # === æ”¹åŠ¨ï¼šä¸ä¾èµ–æ–°é—»æ¥å£ ===
            # æ„å»ºæ–°é—»è®¡æ•°å­—å…¸ï¼ˆå¼ºåˆ¶ä¸ºç©ºï¼Œä¸ä½¿ç”¨ summarise_newsï¼‰
            # The user requested to remove news interface dependency for detection.
            news_counts = {}
            # for news_item in summarized_news or []: ... (Removed)

            # === Redis å…¨å±€ç¼“å­˜æ£€æŸ¥ ===
            redis_client = get_redis()
            cache_key = f"power_zones_v3:{region_code}"
            cached_zones_json = None

            try:
                cached_zones_json = redis_client.get(cache_key)
                if cached_zones_json:
                    import json

                    anomaly_zones = json.loads(cached_zones_json)
                    print(
                        f"[AnomalyZones] âœ“ Using Redis cached {len(anomaly_zones)} zones for {region_code}"
                    )
            except Exception as e:
                print(f"[AnomalyZones] Failed to get Redis client or cache: {e}")

            # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œè®¡ç®—å¹¶ä¿å­˜
            if not cached_zones_json:
                # 1. Trend Analysis (Regime Segmentation)
                trend_service = TrendService()
                # Use all methods but prefer PLR for visual zones
                trend_results = trend_service.analyze_trend(sig_df, method="plr")

                # Debug Prints for Trend Algorithms
                print("\n" + "=" * 50)
                plr_list = trend_results.get("plr", [])
                print(
                    f"\nğŸ“ˆ [ALGO 3/6] Bottom-Up PLR: Found {len(plr_list)} segments. Verifying Continuity:"
                )
                for i, seg in enumerate(plr_list):
                    print(
                        f"   [{i}] {seg['startDate']} -> {seg['endDate']} ({seg['direction']})"
                    )
                print("=" * 50 + "\n")

                # Map PLR segments to anomaly_zones format expected by frontend
                plr_segments = trend_results.get("plr", [])

                # Combine all segments for frontend selection
                all_segments = []
                all_segments.extend(plr_segments)

                # NEW: Generate Semantic Broad Regimes (Merged PLR)
                # This creates broad "Event Flow" phases
                semantic_raw = trend_service.process_semantic_regimes(
                    plr_segments, min_duration_days=7
                )

                # Process semantic zones
                semantic_zones = []
                for seg in semantic_raw:
                    # Determine sentiment/color
                    sentiment = "neutral"
                    direction = seg.get("direction", "").lower()
                    seg_type = seg.get("type", "").lower()

                    if direction == "up" or seg_type == "bull":
                        sentiment = "positive"
                    elif direction == "down" or seg_type == "bear":
                        sentiment = "negative"

                    # Calculate return
                    try:
                        start_p = float(seg.get("startPrice", 0))
                        end_p = float(seg.get("endPrice", 0))
                        change_pct = (end_p - start_p) / start_p if start_p else 0
                    except:
                        change_pct = 0

                    semantic_zones.append(
                        {
                            "startDate": seg["startDate"],
                            "endDate": seg["endDate"],
                            "avg_return": change_pct,
                            "avg_score": abs(change_pct) * 10,
                            "zone_type": "semantic_regime",
                            "method": "plr_merged",
                            "sentiment": sentiment,
                            "summary": f"{seg.get('direction', seg.get('type', 'Phase')).title()} ({change_pct * 100:.1f}%)",
                            "description": f"Phase from {seg['startDate']} to {seg['endDate']}. Return: {change_pct * 100:.1f}%",
                            "type": seg_type,
                            "normalizedType": seg_type,
                            "direction": direction,
                            "events": [],  # Placeholder for events
                        }
                    )

                # Process raw segments (anomaly_zones)
                anomaly_zones = []
                for seg in all_segments:
                    # Determine sentiment/color
                    sentiment = "neutral"
                    direction = seg.get("direction", "").lower()
                    seg_type = seg.get("type", "").lower()

                    if direction == "up" or seg_type == "bull":
                        sentiment = "positive"
                    elif direction == "down" or seg_type == "bear":
                        sentiment = "negative"

                    # Calculate simple impact/score
                    start_p = seg.get("startPrice", seg.get("avgPrice", 1.0))
                    end_p = seg.get("endPrice", seg.get("avgPrice", 1.0))
                    change_pct = (end_p - start_p) / start_p if start_p else 0

                    anomaly_zones.append(
                        {
                            "startDate": seg["startDate"],
                            "endDate": seg["endDate"],
                            "avg_return": change_pct,
                            "avg_score": abs(change_pct) * 10,
                            "zone_type": "trend_segment",
                            "method": seg.get("method", "plr"),
                            "sentiment": sentiment,
                            "summary": f"{seg.get('direction', seg.get('type', 'Trend')).title()} ({change_pct * 100:.1f}%)",
                            "description": f"Trend detected from {seg['startDate']} to {seg['endDate']}. Return: {change_pct * 100:.1f}%",
                            "type": seg_type,
                            "normalizedType": seg_type,
                            "direction": direction,
                        }
                    )

                # Merge semantic zones into anomaly_zones
                anomaly_zones.extend(semantic_zones)

                # Also keep StockSignalService for consistency if needed, but for now we replace the main logic
                # or we can append significant points differently.
                # For this task, we focus on TrendService, but let's keep the existing generated zones logic as specific method 'clustering'?
                # Actually, the user wants to REPLACE/MIGRATE features.
                # Let's keep the old one as "clustering" method if desired, but here we just use TrendService results.
                # However, to avoid losing functionality, we might want to run ClusteringService too?
                # The Plan says "Combine these with existing StockSignalService results or structure them".

                # Let's run StockSignalService as well for 'clustering' method
                clustering_service = StockSignalService(lookback=60, max_zone_days=10)
                clustering_zones = clustering_service.generate_zones(
                    sig_df, news_counts
                )
                for z in clustering_zones:
                    z["method"] = "clustering"

                anomaly_zones.extend(clustering_zones)

                print(
                    f"[AnomalyZones] âš™ï¸ Generated {len(anomaly_zones)} zones (PLR + Semantic + Clustering)"
                )

            # ä¸ºæ¯ä¸ªåŒºåŸŸç”Ÿæˆäº‹ä»¶æ‘˜è¦ï¼ˆå³ä½¿æ˜¯ä»ç¼“å­˜è¯»å–çš„ä¹Ÿå¯ä»¥é‡æ–°ç”Ÿæˆï¼Œæˆ–è€…ä»…å½“æœªç¼“å­˜æ—¶ç”Ÿæˆï¼‰
            if anomaly_zones and not cached_zones_json:
                try:
                    event_agent = EventSummaryAgent()

                    # å¹¶å‘å¤„ç†æ¯ä¸ªZoneçš„æœç´¢æ€»ç»“ (Search REMOVED as per user request)
                    async def process_zone(zone):
                        start = zone["startDate"]
                        end = zone["endDate"]

                        zone_dates = []
                        curr = datetime.strptime(start, "%Y-%m-%d")
                        while curr <= datetime.strptime(end, "%Y-%m-%d"):
                            zone_dates.append(curr.strftime("%Y-%m-%d"))
                            curr += timedelta(days=1)

                        # æ”¹åŠ¨ï¼šä¸å†è°ƒç”¨ Tavily æœç´¢æ–°é—»ä¾›æ‘˜è¦ä½¿ç”¨

                        # ç”Ÿæˆæ‘˜è¦
                        event_summary = event_agent.summarize_zone(
                            zone_dates=zone_dates,
                            price_change=zone.get("avg_return", 0) * 100,
                            news_items=[],  # EMPTY
                            region_name=region_name,
                        )

                        zone["event_summary"] = event_summary
                        # æ”¹åŠ¨ï¼šä¸åœ¨ anomaly zones ä¸­ä¿å­˜ urlï¼Œå› ä¸ºä¸è·å–æ–°é—»äº†
                        zone["news_links"] = []

                        print(
                            f"[AnomalyZones] Zone {start}-{end} (Internal Analysis): {event_summary}"
                        )
                        return zone

                    # å¹¶å‘æ‰§è¡Œ
                    tasks = [process_zone(z) for z in anomaly_zones]
                    anomaly_zones = await asyncio.gather(*tasks)

                except Exception as e:
                    import traceback

                    print(f"[AnomalyZones] Error generating event summaries: {e}")
                    print(traceback.format_exc())
                    # Fallback
                    for zone in anomaly_zones:
                        if "event_summary" not in zone:
                            zone["event_summary"] = (
                                f"ä¾›ç”µé‡æ³¢åŠ¨{zone.get('avg_return', 0) * 100:+.1f}%"
                            )

            # âš ï¸ ä¸å†è¿‡æ»¤æ— æ–°é—»çš„ zonesï¼Œä¿ç•™æ‰€æœ‰æ£€æµ‹åˆ°çš„å¼‚å¸¸åŒºé—´
            anomaly_zones_with_news = anomaly_zones
            print(f"[AnomalyZones] Final zones: {len(anomaly_zones)}")

            # === ä¿å­˜åˆ°Rediså…¨å±€ç¼“å­˜ ===
            if anomaly_zones:
                try:
                    import json

                    zones_json = json.dumps(anomaly_zones, ensure_ascii=False)
                    redis_client.setex(
                        cache_key,
                        12 * 60 * 60,  # 12å°æ—¶TTL
                        zones_json,
                    )
                    print(
                        f"[AnomalyZones] ğŸ’¾ Saved {len(anomaly_zones)} zones to Redis cache (12 hours)"
                    )
                except Exception as e:
                    print(f"[AnomalyZones] Redis cache save error: {e}")

            # ä¿å­˜å¹¶å‘é€å¼‚å¸¸åŒºåŸŸæ•°æ®
            if anomaly_zones:
                message.save_anomaly_zones(anomaly_zones, region_code)

                await self._emit_event(
                    event_queue,
                    message,
                    {
                        "type": "data",
                        "data_type": "anomaly_zones",
                        "data": {"zones": anomaly_zones, "ticker": region_code},
                    },
                )
                print(f"[AnomalyZones] Successfully saved and emitted")

        except Exception as e:
            import traceback

            print(f"[AnomalyZones] Error: {e}")
            print(f"[AnomalyZones] Traceback:\n{traceback.format_exc()}")

        await self._emit_event(
            event_queue,
            message,
            {
                "type": "step_complete",
                "step": 3,
                "data": {"data_points": len(df), "news_count": len(news_items)},
            },
        )
        message.update_step_detail(
            3, "completed", f"å†å²æ•°æ® {len(df)} å¤©, æ–°é—» {len(news_items)} æ¡"
        )

        # === Step 4: åˆ†æå¤„ç†ï¼ˆå¤šå› ç´ å½±å“åŠ›åˆ†æï¼‰===
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_start", "step": 4, "step_name": "åˆ†æå¤„ç†"},
        )
        message.update_step_detail(4, "running", "åˆ†ææ—¶åºç‰¹å¾å’Œå¤šå› ç´ å½±å“åŠ›...")

        # æ—¶åºç‰¹å¾åˆ†æ
        features = await asyncio.to_thread(TimeSeriesAnalyzer.analyze_features, df)

        # å¤šå› ç´ å½±å“åŠ›åˆ†æï¼ˆæ›¿ä»£æƒ…ç»ªåˆ†æï¼‰
        print(
            f"[Influence] å‡†å¤‡åˆ†æå½±å“å› å­ï¼Œä¾›ç”µæ•°æ®: {len(df) if df is not None else 0} æ¡ï¼Œå¤©æ°”æ•°æ®: {len(weather_df) if weather_df is not None else 0} æ¡"
        )
        influence_result = await self._step_influence_analysis(
            df,
            weather_df,
            event_queue,
            message,
            region_match.region_info if region_match else None,
        )
        # print(f"[Influence] å½±å“å› å­åˆ†æå®Œæˆï¼Œç»“æœ: {influence_result}")

        # ä¿å­˜å½±å“å› å­æ•°æ®ï¼ˆå…¼å®¹åŸæœ‰emotionå­—æ®µï¼‰
        message.save_emotion(
            influence_result.get("overall_score", 0),
            influence_result.get("summary")
            or influence_result.get("description", "å½±å“å› ç´ åˆ†æ"),
        )

        await self._emit_event(
            event_queue,
            message,
            {
                "type": "step_complete",
                "step": 4,
                "data": {
                    "trend": features.get("trend", "N/A"),
                    "influence": influence_result.get("summary", "å½±å“å› ç´ åˆ†æ"),
                },
            },
        )
        message.update_step_detail(
            4,
            "completed",
            f"è¶‹åŠ¿: {features.get('trend', 'N/A')}, å½±å“å› ç´ : {influence_result.get('summary', 'åˆ†æå®Œæˆ')}",
        )

        # === Step 5: æ¨¡å‹é¢„æµ‹ ===
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_start", "step": 5, "step_name": "æ¨¡å‹é¢„æµ‹"},
        )
        message.update_step_detail(5, "running", f"è®­ç»ƒæ¨¡å‹...")

        prophet_params = await recommend_forecast_params(
            self.sentiment_agent, influence_result or {}, features
        )

        # ä½¿ç”¨æ„å›¾ä¸­çš„é¢„æµ‹å¤©æ•°ï¼ˆé»˜è®¤30å¤©ï¼‰
        forecast_horizon = max(intent.forecast_horizon, 1)

        # æ¨¡å‹é€‰æ‹©ï¼šé»˜è®¤ä½¿ç”¨åŸºäºå†å²åŒæœŸæ•°æ®çš„é¢„æµ‹æ–¹æ³•
        # è¯¥æ–¹æ³•åŸºäºè¿‘2å¹´åŒæœŸæ•°æ®å¹³å‡ï¼Œå¹¶æ ¹æ®å¤©æ°”å·®å¼‚è°ƒæ•´
        user_specified_model = intent.forecast_model

        # é»˜è®¤ä½¿ç”¨å†å²åŒæœŸé¢„æµ‹æ–¹æ³•
        if not user_specified_model or user_specified_model == "auto":
            final_model = "historical_average"
            model_selection_reason = "åŸºäºè¿‘2å¹´å†å²åŒæœŸæ•°æ®å¹³å‡é¢„æµ‹ï¼Œå¹¶æ ¹æ®å¤©æ°”å·®å¼‚è°ƒæ•´"
        else:
            # ç”¨æˆ·æŒ‡å®šäº†æ¨¡å‹ï¼Œä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ¨¡å‹
            final_model = user_specified_model
            model_selection_reason = (
                f"ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„ {user_specified_model.upper()} æ¨¡å‹"
            )

        # å‘é€æ¨¡å‹é€‰æ‹©äº‹ä»¶ï¼ˆç®€åŒ–ç‰ˆï¼‰
        await self._emit_event(
            event_queue,
            message,
            {
                "type": "model_selection",
                "selected_model": final_model,
                "best_model": final_model,
                "baseline": "seasonal_naive",
                "model_comparison": {},
                "is_better_than_baseline": False,
                "user_specified_model": user_specified_model,
                "model_selection_reason": model_selection_reason,
            },
        )

        # ä¿å­˜æ¨¡å‹é€‰æ‹©ä¿¡æ¯åˆ° Message
        message.save_model_selection(final_model, {}, False)

        # ä¿å­˜æ¨¡å‹é€‰æ‹©åŸå› 
        message.save_model_selection_reason(model_selection_reason)

        message.update_step_detail(5, "running", f"è®­ç»ƒ {final_model.upper()} æ¨¡å‹...")

        prophet_params = await recommend_forecast_params(
            self.sentiment_agent, influence_result or {}, features
        )

        # åªå¯¹æœ€ç»ˆé€‰å®šçš„æ¨¡å‹è°ƒç”¨ä¸€æ¬¡ run_forecast
        forecast_result = await run_forecast(
            df, final_model, max(forecast_horizon, 1), prophet_params, weather_df, region_name
        )

        # ä¿å­˜å¹¶å‘é€é¢„æµ‹ç»“æœï¼ˆforecast_result æ˜¯ ForecastResult å¯¹è±¡ï¼‰
        full_points = original_points + forecast_result.points
        prediction_start = (
            forecast_result.points[0].date if forecast_result.points else ""
        )
        message.save_time_series_full(full_points, prediction_start)

        await self._emit_event(
            event_queue,
            message,
            {
                "type": "data",
                "data_type": "time_series_full",
                "data": [p.model_dump() for p in full_points],
                "prediction_start_day": prediction_start,
            },
        )

        metrics = forecast_result.metrics
        metrics_dict = {"mae": metrics.mae}
        if metrics.rmse:
            metrics_dict["rmse"] = metrics.rmse
        metrics_info = f"MAE: {metrics.mae}" + (
            f", RMSE: {metrics.rmse}" if metrics.rmse else ""
        )
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_complete", "step": 5, "data": {"metrics": metrics_dict}},
        )
        message.update_step_detail(5, "completed", f"é¢„æµ‹å®Œæˆ ({metrics_info})")

        # ä¿å­˜æ¨¡å‹åç§°åˆ° MessageDataï¼ˆä½¿ç”¨æœ€ç»ˆé€‰å®šçš„æ¨¡å‹ï¼‰
        message.save_model_name(final_model)

        # === Change Point Detection & Analysis (Separated History / Forecast) ===
        try:
            # 1. å‡†å¤‡æ•°æ®ï¼šåˆ†ç¦»å†å²å’Œé¢„æµ‹
            hist_points = [p for p in full_points if not p.is_prediction]
            pred_points = [p for p in full_points if p.is_prediction]

            # å¦‚æœé¢„æµ‹ç‚¹ä¸è¶³ï¼Œå¯èƒ½æ˜¯çº¯å†å²åˆ†ææˆ–é¢„æµ‹æœªå¼€å§‹
            if not pred_points and len(hist_points) > 0:
                # å‡è®¾æœ€åä¸€éƒ¨åˆ†æ˜¯å…¶å®æ˜¯æœªæ¥é¢„æµ‹ï¼ˆé’ˆå¯¹æŸäº›ç‰¹æ®Šcaseï¼‰ï¼Œæˆ–è€…å¹²è„†ä¸æ£€æµ‹æœªæ¥
                pass

            import pandas as pd

            # å®šä¹‰æ£€æµ‹å‡½æ•°ï¼Œæ–¹ä¾¿å¤ç”¨
            def run_detection(points, label, threshold):
                if not points:
                    return []
                df = pd.DataFrame(
                    [
                        {"date": p.date, "y": p.value, "is_prediction": p.is_prediction}
                        for p in points
                    ]
                )
                print(
                    f"[ChangePoints] Starting detection for {region_name} on {label} data ({len(df)} rows)"
                )
                # å¿…é¡»é‡æ–°ç´¢å¼•ï¼Œå¦åˆ™ç´¢å¼•ä¼šä¸è¿ç»­å½±å“æ£€æµ‹é€»è¾‘ï¼ˆå¦‚æœdetectå†…éƒ¨ä¾èµ–ç´¢å¼•è¿ç»­æ€§ï¼‰
                df = df.reset_index(drop=True)

                srv = StockSignalService()
                return srv.detect_change_points(df, threshold=threshold)

            # åˆ†åˆ«æ£€æµ‹
            # å†å²æ•°æ®é€šå¸¸å™ªå£°è¾ƒå¤§ï¼Œå¯ä»¥ä½¿ç”¨ç¨é«˜é˜ˆå€¼ï¼›æˆ–è€…ä¿æŒä¸€è‡´
            hist_cps = run_detection(hist_points, "HISTORY", threshold=1.3)
            for cp in hist_cps:
                cp["is_prediction"] = False

            # é¢„æµ‹æ•°æ®é€šå¸¸è¾ƒå¹³æ»‘ï¼Œé˜ˆå€¼å¯ä½ä¸€ç‚¹ä»¥æ•æ„Ÿæ•æ‰
            pred_cps = run_detection(pred_points, "FORECAST", threshold=1.2)
            for cp in pred_cps:
                cp["is_prediction"] = True

            all_change_points = hist_cps + pred_cps
            print(
                f"[ChangePoints] Total detected: {len(all_change_points)} (Hist: {len(hist_cps)}, Pred: {len(pred_cps)})"
            )

            if all_change_points:
                analyzed_points = []

                # é¢„å¤„ç†å¤©æ°”æ•°æ®æŸ¥æ‰¾è¡¨
                weather_lookup = {}
                if weather_df is not None and not weather_df.empty:
                    try:
                        weather_df["date_str"] = (
                            weather_df["date"].astype(str).str.slice(0, 10)
                        )
                        for _, row in weather_df.iterrows():
                            temp = f"{row.get('temperature', 'N/A')}Â°C"
                            hum = f"æ¹¿åº¦{row.get('humidity', 'N/A')}%"
                            weather_lookup[row["date_str"]] = f"{temp}, {hum}"
                    except Exception as e:
                        print(f"[ChangePoints] Weather lookup build error: {e}")

                # å¼‚æ­¥æœç´¢å·¥å…·å‡½æ•°
                async def enrich_point(cp):
                    cp_date = cp.get("date")
                    is_pred = cp.get("is_prediction", False)

                    # 2. ä¸Šä¸‹æ–‡æ„å»º
                    context_info = []
                    w_info = weather_lookup.get(cp_date)
                    if w_info:
                        context_info.append(f"å¤©æ°”: {w_info}")

                    if is_pred:
                        context_info.append("(æœªæ¥é¢„æµ‹)")
                    else:
                        context_info.append("(å†å²æ•°æ®)")

                    weather_info_str = " ".join(context_info)

                    # 3. å¹¶è¡Œæ‰§è¡Œï¼šLLMåˆ†æ + Tavilyæœç´¢

                    # LLM åˆ†æä»»åŠ¡
                    llm_task = asyncio.to_thread(
                        self.prediction_analysis_agent.analyze_change_point,
                        cp,
                        region_name,
                        weather_info_str,
                    )

                    # Tavily æœç´¢ä»»åŠ¡ (ä»…å¯¹å†å²ç‚¹æˆ–è¿‘æœŸæœªæ¥ç‚¹æ›´æœ‰æ„ä¹‰)
                    search_task = None
                    if not is_pred:  # å†å²æ•°æ®æ‰æœç´¢æ–°é—»
                        keywords = [region_name, "ä¾›ç”µ", "å¤©æ°”", "å·¥ä¸š"]
                        # ä½¿ç”¨ç‰¹å®šæ—¥æœŸçš„æœç´¢
                        search_task = search_news_around_date(
                            keywords, target_date=cp_date, days=3, max_results=3
                        )

                    # æ‰§è¡Œä»»åŠ¡
                    analysis_res, search_res = await asyncio.gather(
                        llm_task,
                        search_task if search_task else asyncio.sleep(0),
                        return_exceptions=True,
                    )

                    # å¤„ç†ç»“æœ
                    cp["reason"] = (
                        analysis_res if isinstance(analysis_res, str) else "åˆ†æå¤±è´¥"
                    )

                    # å¤„ç†æœç´¢ç»“æœ
                    news_links = []
                    if isinstance(search_res, list):
                        for item in search_res:
                            news_links.append(
                                {
                                    "title": item.get("title", f"ç›¸å…³æ–°é—» ({cp_date})"),
                                    "url": item.get("url", "#"),
                                    "source": extract_domain(item.get("url", "")),
                                }
                            )
                    cp["news_links"] = news_links

                    # æ„å»ºå¤©æ°”é“¾æ¥ (é€šç”¨æœç´¢é“¾æ¥)
                    weather_query = f"{region_name} {cp_date} å¤©æ°”"
                    cp["weather_link"] = (
                        f"https://www.bing.com/search?q={weather_query}"
                    )

                    return cp

                    cp["is_prediction"] = is_pred

                    return cp

                # å¹¶å‘å¤„ç†æ‰€æœ‰ç‚¹
                # é™åˆ¶å¹¶å‘æ•°ä»¥é˜²è§¦å‘APIé€Ÿç‡é™åˆ¶
                limit = asyncio.Semaphore(5)

                async def sem_task(cp):
                    async with limit:
                        return await enrich_point(cp)

                tasks = [sem_task(cp) for cp in all_change_points]
                analyzed_points = await asyncio.gather(*tasks)

                # Emit event
                await self._emit_event(
                    event_queue,
                    message,
                    {
                        "type": "data",
                        "data_type": "change_points",
                        "data": analyzed_points,
                    },
                )
                message.save_change_points(analyzed_points)

        except Exception as e:
            print(f"âŒ Change Point Analysis Error: {e}")
            import traceback

            print(traceback.format_exc())

        # === Step 6: æŠ¥å‘Šç”Ÿæˆï¼ˆæµå¼ï¼‰ ===
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_start", "step": 6, "step_name": "æŠ¥å‘Šç”Ÿæˆ"},
        )
        message.update_step_detail(6, "running", "ç”Ÿæˆåˆ†ææŠ¥å‘Š...")

        # å°† ForecastResult è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ä¾›æŠ¥å‘Šç”Ÿæˆä½¿ç”¨
        forecast_dict = {
            "forecast": [
                {"date": p.date, "value": p.value} for p in forecast_result.points
            ],
            "metrics": metrics_dict,
            "model": forecast_result.model,
        }

        report_content = await self._step_report_streaming(
            user_input,
            features,
            forecast_dict,
            influence_result or {},  # ä½¿ç”¨å½±å“å› å­ç»“æœæ›¿ä»£æƒ…ç»ªç»“æœ
            conversation_history,
            event_queue,
            message,
        )

        message.save_conclusion(report_content)
        await self._emit_event(
            event_queue, message, {"type": "step_complete", "step": 6, "data": {}}
        )
        message.update_step_detail(6, "completed", "æŠ¥å‘Šç”Ÿæˆå®Œæˆ")

    # ========== èŠå¤©æµç¨‹ï¼ˆæµå¼ï¼‰ ==========

    async def _execute_chat_streaming(
        self,
        message: Message,
        _session: Session,  # ä¿ç•™å‚æ•°ä»¥ä¿æŒæ¥å£ä¸€è‡´æ€§
        user_input: str,
        intent: UnifiedIntent,
        region_match: Optional[RegionMatchResult],
        keywords: ResolvedKeywords,
        conversation_history: List[dict],
        event_queue: asyncio.Queue | None,
    ):
        """æµå¼èŠå¤©æµç¨‹"""
        # ä¼˜å…ˆä½¿ç”¨region_mentionï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨stock_mentionï¼ˆå…¼å®¹ï¼‰
        region_mention = intent.region_mention or intent.stock_mention
        step_num = 3 if region_mention else 2

        # === æ•°æ®è·å– ===
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_start", "step": step_num, "step_name": "ä¿¡æ¯è·å–"},
        )
        message.update_step_detail(step_num, "running", "è·å–ç›¸å…³ä¿¡æ¯...")

        tasks = []
        task_names = []

        if intent.enable_rag:
            rag_available = await check_rag_availability()
            if rag_available:
                tasks.append(
                    fetch_rag_reports(self.rag_searcher, keywords.rag_keywords)
                )
                task_names.append("rag")

        if intent.enable_search:
            tasks.append(search_web(keywords.search_keywords, intent.history_days))
            task_names.append("search")

        if intent.enable_domain_info:
            region_name = (
                region_match.region_info.region_name
                if region_match and region_match.region_info
                else ""
            )
            tasks.append(fetch_domain_news(region_name, keywords.domain_keywords))
            task_names.append("domain")

        results = {}
        if tasks:
            task_results = await asyncio.gather(*tasks, return_exceptions=True)
            for name, result in zip(task_names, task_results):
                if not isinstance(result, Exception):
                    results[name] = result

        await self._emit_event(
            event_queue,
            message,
            {
                "type": "step_complete",
                "step": step_num,
                "data": {"sources": list(results.keys())},
            },
        )
        message.update_step_detail(
            step_num, "completed", f"è·å–å®Œæˆ: {list(results.keys())}"
        )

        # === ç”Ÿæˆå›ç­”ï¼ˆæµå¼ï¼‰ ===
        step_num += 1
        await self._emit_event(
            event_queue,
            message,
            {"type": "step_start", "step": step_num, "step_name": "ç”Ÿæˆå›ç­”"},
        )
        message.update_step_detail(step_num, "running", "ç”Ÿæˆå›ç­”...")

        # æ„å»ºä¸Šä¸‹æ–‡
        context_parts = []

        if "rag" in results and results["rag"]:
            context_parts.append("=== ç ”æŠ¥å†…å®¹ ===")
            for source in results["rag"][:5]:
                context_parts.append(
                    f"[{source.filename} ç¬¬{source.page}é¡µ]: {source.content_snippet}"
                )

        if "search" in results and results["search"]:
            context_parts.append("\n=== ç½‘ç»œæœç´¢ ===")
            for item in results["search"][:5]:
                context_parts.append(
                    f"[{item.get('title', '')}]({item.get('url', '')}): {item.get('content', '')[:100]}"
                )

        if "domain" in results and results["domain"]:
            context_parts.append("\n=== å³æ—¶æ–°é—» ===")
            for item in results["domain"][:5]:
                title = item.get("title", "")
                url = item.get("url", "")
                content = item.get("content", "")[:100]
                if url:
                    context_parts.append(f"[{title}]({url}): {content}")
                else:
                    context_parts.append(f"- {title}: {content}")

        context = "\n".join(context_parts) if context_parts else ""

        # æµå¼ç”Ÿæˆå›ç­”
        answer = await self._step_chat_streaming(
            user_input, conversation_history, context, event_queue, message
        )

        message.save_conclusion(answer)

        if "rag" in results:
            message.save_rag_sources(results["rag"])

        await self._emit_event(
            event_queue,
            message,
            {"type": "step_complete", "step": step_num, "data": {}},
        )
        message.update_step_detail(step_num, "completed", "å›ç­”å®Œæˆ")

    # ========== æµå¼æŠ¥å‘Šç”Ÿæˆ ==========

    async def _step_report_streaming(
        self,
        user_input: str,
        features: Dict,
        forecast_result: Dict,
        emotion_result: Dict,
        conversation_history: List[dict],
        event_queue: asyncio.Queue | None,
        message: Message,
    ) -> str:
        """æµå¼æŠ¥å‘Šç”Ÿæˆ"""
        loop = asyncio.get_running_loop()
        content_queue: asyncio.Queue = asyncio.Queue()

        def run_in_thread():
            def on_chunk(chunk: str):
                loop.call_soon_threadsafe(content_queue.put_nowait, ("chunk", chunk))

            content = self.report_agent.generate_streaming(
                user_input,
                features,
                forecast_result,
                emotion_result,
                conversation_history,
                on_chunk,
            )
            loop.call_soon_threadsafe(content_queue.put_nowait, ("done", content))

        future = loop.run_in_executor(None, run_in_thread)

        full_content = ""
        while True:
            try:
                event_type, data = await asyncio.wait_for(
                    content_queue.get(), timeout=120.0
                )

                if event_type == "chunk":
                    full_content += data
                    await self._emit_event(
                        event_queue,
                        message,
                        {"type": "report_chunk", "content": full_content},
                    )
                elif event_type == "done":
                    full_content = data
                    break
            except asyncio.TimeoutError:
                break

        await future

        return full_content

    # ========== å¤šå› ç´ å½±å“åŠ›åˆ†æ ==========

    async def _step_influence_analysis(
        self,
        power_df: pd.DataFrame,
        weather_df: Optional[pd.DataFrame],
        event_queue: asyncio.Queue | None,
        message: Message,
        region_info: Optional[Any] = None,
    ) -> Dict[str, Any]:
        """å¤šå› ç´ å½±å“åŠ›åˆ†æï¼ˆæ›¿ä»£æƒ…ç»ªåˆ†æï¼‰"""

        # å¦‚æœæ²¡æœ‰ä¾›ç”µæ•°æ®æˆ–å¤©æ°”æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼
        if power_df is None or power_df.empty or weather_df is None or weather_df.empty:
            default_result = InfluenceAnalyzer._get_default_result()
            default_result["overall_score"] = 0.0
            print(f"[Influence] æ•°æ®ä¸è¶³ï¼Œå‘é€é»˜è®¤å½±å“å› å­")
            await self._emit_event(
                event_queue,
                message,
                {
                    "type": "data",
                    "data_type": "influence",
                    "data": default_result,
                },
            )
            return default_result

        # è·å–æ—¥æœŸèŒƒå›´
        start_date = power_df["ds"].min()
        end_date = power_df["ds"].max()

        # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼ˆç§»é™¤æ—¶åŒºï¼‰
        if hasattr(start_date, "tz") and start_date.tz is not None:
            start_date = start_date.tz_localize(None)
        if hasattr(end_date, "tz") and end_date.tz is not None:
            end_date = end_date.tz_localize(None)

        start_date_str = start_date.strftime("%Y-%m-%d")
        end_date_str = end_date.strftime("%Y-%m-%d")

        # åˆ›å»ºç©ºçš„èŠ‚å‡æ—¥æ•°æ®ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»¥å…¼å®¹æ¥å£ï¼‰
        holiday_df = pd.DataFrame(columns=["date", "is_holiday", "holiday_score"])

        # è·å–åŸå¸‚å·¥ä¸šç»“æ„æ•°æ®
        industry_structure_ratio = 0.3  # é»˜è®¤å€¼ï¼ˆå…¨å›½å¹³å‡å·¥ä¸šç»“æ„æ¯”ä¾‹çº¦30%ï¼‰
        if region_info and region_info.region_name:
            industry_structure_client = get_industry_structure_client()
            try:
                # è°ƒç”¨LLMè·å–åŸå¸‚å·¥ä¸šç»“æ„æ•°æ®ï¼ˆéasyncå‡½æ•°ï¼Œä½¿ç”¨çº¿ç¨‹æ‰§è¡Œï¼‰
                structure_data = await asyncio.to_thread(
                    industry_structure_client.fetch_industry_structure_data,
                    region_info.region_name,
                )
                industry_structure_ratio = structure_data.get(
                    "second_industry_ratio", 0.3
                )
                print(
                    f"[Influence] è·å–åŸå¸‚å·¥ä¸šç»“æ„æ•°æ®: {region_info.region_name}, æ¯”ä¾‹={industry_structure_ratio:.2%}"
                )
            except Exception as e:
                print(f"[Influence] è·å–åŸå¸‚å·¥ä¸šç»“æ„æ•°æ®å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤å€¼0.3")
        else:
            print(f"[Influence] æœªæä¾›åŸå¸‚åç§°ï¼Œä½¿ç”¨é»˜è®¤å·¥ä¸šç»“æ„æ¯”ä¾‹0.3")

        # è®¡ç®—å½±å“å› å­ï¼ˆä½¿ç”¨æ–°æ–¹æ³•ï¼‰
        influence_result = await asyncio.to_thread(
            InfluenceAnalyzer.analyze_factors_influence,
            power_df,
            weather_df,
            holiday_df,
            industry_structure_ratio,
        )

        # è®¡ç®—æ€»ä½“å¾—åˆ†ï¼ˆå„å› ç´ å½±å“åŠ›å¾—åˆ†çš„å¹³å‡å€¼ï¼Œè¿‡æ»¤NaNå€¼ï¼‰
        if influence_result.get("ranking"):
            valid_scores = [
                factor["influence_score"]
                for factor in influence_result["ranking"]
                if not (
                    np.isnan(factor.get("influence_score", np.nan))
                    or np.isinf(factor.get("influence_score", np.nan))
                )
            ]
            overall_score = np.mean(valid_scores) if valid_scores else 0.0
        else:
            overall_score = 0.0

        # ç¡®ä¿overall_scoreä¸æ˜¯NaN
        if np.isnan(overall_score) or np.isinf(overall_score):
            overall_score = 0.0

        influence_result["overall_score"] = round(float(overall_score), 4)

        # ä¿å­˜å½±å“å› å­æ•°æ®åˆ° Redis
        message.save_influence_analysis(influence_result)

        print(
            f"[Influence] å‘é€å½±å“å› å­æ•°æ®: {len(influence_result.get('ranking', []))} ä¸ªå› å­"
        )
        await self._emit_event(
            event_queue,
            message,
            {
                "type": "data",
                "data_type": "influence",
                "data": influence_result,
            },
        )
        print(f"[Influence] å½±å“å› å­æ•°æ®å·²å‘é€å¹¶ä¿å­˜åˆ°Redis")

        return influence_result

    # ========== æµå¼æƒ…ç»ªåˆ†æï¼ˆä¿ç•™ä»¥å…¼å®¹ï¼‰==========

    async def _step_sentiment_streaming(
        self,
        news_items: List[SummarizedNewsItem],
        event_queue: asyncio.Queue | None,
        message: Message,
    ) -> Dict[str, Any]:
        """æµå¼æƒ…ç»ªåˆ†æ"""
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        news_list = (
            [
                {
                    "title": n.summarized_title,
                    "content": n.summarized_content,
                    "source_name": n.source_name,
                    "source_type": n.source_type,
                }
                for n in news_items
            ]
            if news_items
            else []
        )

        if not news_list:
            # æ— æ–°é—»æ•°æ®ï¼Œè¿”å›é»˜è®¤å€¼
            default_desc = "æ— æ–°é—»æ•°æ®ï¼Œé»˜è®¤ä¸­æ€§æƒ…ç»ª"
            await self._emit_event(
                event_queue,
                message,
                {
                    "type": "data",
                    "data_type": "emotion",
                    "data": {"score": 0.0, "description": default_desc},
                },
            )
            return {"score": 0.0, "description": default_desc}

        loop = asyncio.get_running_loop()
        content_queue: asyncio.Queue = asyncio.Queue()
        result_holder = {"result": None}

        def run_in_thread():
            def on_chunk(chunk: str):
                loop.call_soon_threadsafe(content_queue.put_nowait, ("chunk", chunk))

            result = self.sentiment_agent.analyze_streaming(news_list, on_chunk)
            result_holder["result"] = result
            loop.call_soon_threadsafe(content_queue.put_nowait, ("done", None))

        future = loop.run_in_executor(None, run_in_thread)

        # å®æ—¶å‘é€æƒ…ç»ªæè¿°
        description_buffer = ""
        score_sent = False

        while True:
            try:
                event_type, data = await asyncio.wait_for(
                    content_queue.get(), timeout=60.0
                )

                if event_type == "chunk":
                    description_buffer += data
                    # æµå¼å‘é€ï¼ˆscore å…ˆè®¾ä¸º 0ï¼Œç­‰å®Œæˆåæ›´æ–°ï¼‰
                    if not score_sent:
                        score_sent = True
                    await self._emit_event(
                        event_queue,
                        message,
                        {"type": "emotion_chunk", "content": description_buffer},
                    )
                elif event_type == "done":
                    break
            except asyncio.TimeoutError:
                break

        await future

        # è·å–æœ€ç»ˆç»“æœ
        result = result_holder["result"] or {
            "score": 0.0,
            "description": description_buffer or "ä¸­æ€§æƒ…ç»ª",
        }

        # å‘é€æœ€ç»ˆæƒ…ç»ªæ•°æ®
        await self._emit_event(
            event_queue,
            message,
            {
                "type": "data",
                "data_type": "emotion",
                "data": {
                    "score": result["score"],
                    "description": result["description"],
                },
            },
        )

        return result

    # ========== æµå¼èŠå¤©ç”Ÿæˆ ==========

    async def _step_chat_streaming(
        self,
        user_input: str,
        conversation_history: List[dict],
        context: str,
        event_queue: asyncio.Queue | None,
        message: Message,
    ) -> str:
        """æµå¼èŠå¤©ç”Ÿæˆ"""
        loop = asyncio.get_running_loop()
        content_queue: asyncio.Queue = asyncio.Queue()

        def run_in_thread():
            gen = self.intent_agent.generate_chat_response(
                user_input, conversation_history, context, stream=True
            )
            full = ""
            for chunk in gen:
                full += chunk
                loop.call_soon_threadsafe(content_queue.put_nowait, ("chunk", full))
            loop.call_soon_threadsafe(content_queue.put_nowait, ("done", full))

        future = loop.run_in_executor(None, run_in_thread)

        full_content = ""
        while True:
            try:
                event_type, data = await asyncio.wait_for(
                    content_queue.get(), timeout=120.0
                )

                if event_type == "chunk":
                    full_content = data
                    await self._emit_event(
                        event_queue,
                        message,
                        {"type": "chat_chunk", "content": full_content},
                    )
                elif event_type == "done":
                    full_content = data
                    break
            except asyncio.TimeoutError:
                break

        await future
        return full_content

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _update_stream_status(self, message: Message, status: str):
        """æ›´æ–°æµå¼çŠ¶æ€"""
        data = message.get()
        if data:
            data.stream_status = status
            message._save(data)

    def _clean_nan_values(self, obj):
        """é€’å½’æ¸…ç†å­—å…¸å’Œåˆ—è¡¨ä¸­çš„NaNå€¼ï¼Œè½¬æ¢ä¸ºNone"""
        if isinstance(obj, dict):
            return {k: self._clean_nan_values(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._clean_nan_values(item) for item in obj]
        elif isinstance(obj, float):
            if np.isnan(obj) or np.isinf(obj):
                return None
            return obj
        elif isinstance(obj, np.floating):
            if np.isnan(obj) or np.isinf(obj):
                return None
            return float(obj)
        elif isinstance(obj, np.integer):
            return int(obj)
        else:
            return obj

    async def _emit_event(
        self, event_queue: asyncio.Queue | None, message: Message, event: Dict
    ):
        """å‘é€äº‹ä»¶åˆ°é˜Ÿåˆ—ã€PubSub å’Œ Stream"""

        # æ¸…ç†NaNå€¼ä»¥ä¾¿JSONåºåˆ—åŒ–
        event_clean = self._clean_nan_values(event)

        # 1. å‘é€åˆ°æœ¬åœ°é˜Ÿåˆ—ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if event_queue:
            await event_queue.put(event_clean)

        try:
            # 2. å³æ—¶å‘å¸ƒåˆ° PubSub
            channel = f"stream:{message.message_id}"
            json_payload = json.dumps(event_clean, ensure_ascii=False)
            self.redis.publish(channel, json_payload)

            # 3. æŒä¹…åŒ–åˆ° Streamï¼ˆä¾›æ–­ç‚¹ç»­ä¼ ä½¿ç”¨ï¼‰
            stream_key = f"stream-events:{message.message_id}"
            self.redis.xadd(
                stream_key, {"data": json_payload}, maxlen=1000, approximate=True
            )
            self.redis.expire(stream_key, 86400)  # 24å°æ—¶ TTL

        except Exception as e:
            print(f"[StreamingTask] Event storage error: {e}")

    async def _emit_error(
        self, event_queue: asyncio.Queue | None, message: Message, error_msg: str
    ):
        """å‘é€é”™è¯¯äº‹ä»¶"""
        await self._emit_event(
            event_queue, message, {"type": "error", "message": error_msg}
        )

    async def _emit_done(self, event_queue: asyncio.Queue | None, message: Message):
        """å‘é€å®Œæˆäº‹ä»¶"""
        await self._emit_event(
            event_queue, message, {"type": "done", "completed": True}
        )


# å•ä¾‹
_streaming_processor: Optional[StreamingTaskProcessor] = None


def get_streaming_processor() -> StreamingTaskProcessor:
    """è·å–æµå¼ä»»åŠ¡å¤„ç†å™¨å•ä¾‹"""
    global _streaming_processor
    if _streaming_processor is None:
        _streaming_processor = StreamingTaskProcessor()
    return _streaming_processor
